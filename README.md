# spark_sql_python

## Table of Contents

- [General diagram](#general-diagram)
- [Spark Fundamentals & Setting Up](#Spark-Fundamentals-&-Setting-Up)
- [Data Ingestion & Schema Evolution](#Data-Ingestion-&-Schema-Evolution)
- [Data Exploration & Manipulation](#Data-Exploration-&-Manipulation)
- [Advance Query Optimization](#Advance-Query-Optimization)
- [Working With External Data Sources](#Working-With-External-Data-Sources)
- [Data Pipeline & Best Practices](#Data-Pipeline-&-Best-Practices)

## General Diagram

1. identify the object - hypothesis - ok
2. identify the information i am going to use 
3. identify the steps to merge the data
4. identify the technique to show the result
5. identify the way to apply the insight found
6. collect the datasets
7. clean the datasets
8. creating the business rules based in steps to merge
9. show the result based in the technique 
10. apply the changes

## topic : energy
## hypothesis: deploy machine with hydrogen in country places will
### generate better lives styles and it is an opportunity to fight 
### against the corruptions
## information: 
1. Who is the client, what business domain is the client in?
   - the client should be my father because he has the product, but my fathers client
     - should be every place without easy accessing to energy
2. What business problem are we trying to address?
   - expensive energy in poor places
   - real situation where the energy does not exist
   - places withou easy access to the energy
3. How is it going to be consumed by the customer?
   - we will be able to prepare a better strategy to deploy the hydrogen generator
4. What is the economic impact of this project?
   - places without easy access to energy will be able to :
     - cook, get warm, warm shower, live better
   - people break the government dependency in term of energy
5. What type of decisions will our data science feature drive?
   - 
6. What metric will we use to call this project a success and how will we measure it?
#

## Spark Fundamentals & Setting Up


## Data Ingestion & Schema Evolution


## Data Exploration & Manipulation


## Advance Query Optimization


## Working With External Data Sources


## Data pipeline & Best Practices